# -*- coding: utf-8 -*-
"""Untitled115.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JL6RSmYbH2PSQUi3nlvQ8EySHy_LfRp
"""

import os
import pickle
import faiss
import numpy as np
import requests
import streamlit as st
from sentence_transformers import SentenceTransformer

# ---------- 1. Load data & models (cached) ----------
@st.cache_resource(show_spinner=False)
def load_resources():
    index = faiss.read_index("cse_index.faiss")
    with open("cse_chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    embedder = SentenceTransformer("BAAI/bge-small-en-v1.5")
    return index, chunks, embedder

index, chunks, embedder = load_resources()

# ---------- 2. Helper functions ----------
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]          # stored in secrets.toml
GROQ_URL      = "https://api.groq.com/openai/v1/chat/completions"
MODEL_NAME    = "llama3-8b-8192"

def retrieve_context(query, k=2):
    query_vec = embedder.encode([query])
    D, I = index.search(np.array(query_vec), k)
    return "\n".join(chunks[i] for i in I[0])

def call_groq(system_prompt, user_prompt):
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type":  "application/json"
    }
    body = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user",   "content": user_prompt}
        ],
        "temperature": 0.4,
        "max_tokens": 800
    }
    resp = requests.post(GROQ_URL, headers=headers, json=body, timeout=30)
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]
    
# Format memory list as bullet points
memory_list = st.session_state.user_prompts[:]  # want all memory
formatted_memory = "\n".join(f"- {q}" for q in memory_list) 

def generate_answer(user_msg):
    context = retrieve_context(user_msg)

    # Format memory list as bullet points
    memory_list = st.session_state.user_prompts[:]  # keep full memory
    formatted_memory = "\n".join(f"- {q}" for q in memory_list) if memory_list else "None yet."

    system_prompt = (
        "You are FYATâ€¯AIâ€¯1.0, a helpful BRAC University CSE assistant.\n\n"
        "First, understand the user's query. Then answer based on the context below. "
        "Fix grammar and formatting in your response for professionalism.\n\n"
        f"{context}\n\n"
        "If the context seems insufficient, respond with your pretrained knowledge **but you must inform the user that you're using your own knowledge**. "
        "Also, politely guide the user to:\n"
        "https://cse.sds.bracu.ac.bd/ and https://www.bracu.ac.bd/.\n\n"
        "Previous prompts sent by the user (refer if useful):\n"
        f"{formatted_memory}"
    )

    # Prepare full message history
    messages = [{"role": "system", "content": system_prompt}]
    
    history = st.session_state.history[:-1] if len(st.session_state.history) else []
    for i in range(0, len(history), 2):
        user = history[i][1]
        bot = history[i+1][1] if i+1 < len(history) else ""
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})

    messages.append({"role": "user", "content": user_msg})

    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    body = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.4,
        "max_tokens": 800
    }

    response = requests.post(GROQ_URL, headers=headers, json=body, timeout=30)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]


# ---------- 3. Streamlit UI ----------
st.set_page_config(page_title="FYATâ€¯AI â€“ BRACUâ€¯CSE Assistant", page_icon="ðŸª„")

st.title("ðŸª„ FYATâ€¯AI: BRACUâ€¯CSE Knowledge Assistant")
st.caption("Powered by BAAI embeddings + Llamaâ€¯3â€‘8B (GROQ)")

with st.expander("Disclaimer", expanded=False):
    st.markdown(
        "This tool provides information for educational purposes only and is **not** a "
        "substitute for FYAT Mentors or official university resources."
    )

# Chat history lives in session_state
if "history" not in st.session_state:
    st.session_state.history = []

if "user_prompts" not in st.session_state:
    st.session_state.user_prompts = []


# Display past messages
for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# User input box
user_msg = st.chat_input("Ask me anything about BRACUâ€¯CSEâ€¦")

if user_msg:
    st.chat_message("user").markdown(user_msg)
    with st.chat_message("assistant"):
        with st.spinner("Thinkingâ€¦"):
            answer = generate_answer(user_msg)
            st.markdown(answer)
    # Update history
    st.session_state.history.append(("user", user_msg))
    st.session_state.user_prompts.append(user_msg)

    st.session_state.history.append(("assistant", answer))

# optiional: Show Past Prompts in Sidebar
with st.sidebar:
    st.markdown("### ðŸ§  Prompt Memory")
    if st.session_state.user_prompts:
        for i, q in enumerate(st.session_state.user_prompts, 1):
            st.markdown(f"{i}. {q}")
    else:
        st.markdown("_No prompts yet._")

